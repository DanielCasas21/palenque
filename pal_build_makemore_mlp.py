# -*- coding: utf-8 -*-
"""pal_build_makemore_mlp.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RP08Qtc-ORjr8wrEZWQOmZn9em3ROzgh

Daniel J Casas
NLP Universidad Pompeu Fabra 2024 MLTA
"""

from google.colab import drive
drive.mount('/content/drive')

with open('/content/drive/MyDrive/NLP/neural_networks/corpus_all_2.txt', 'r') as file:
    words_corp = file.read().splitlines()

# Remove empty strings from the list
#words = [word for word in words if word.strip()]

# Remove empty strings and strip whitespace from the end of each word
words_corpus = [word.strip() for word in words_corp if word.strip()]

len(words_corpus)

#import csv

#file_path = '/content/drive/MyDrive/NLP/neural networks/k_words.csv'
#words = []

# Read CSV file and append non-empty words
#with open(file_path, 'r') as file:
#    csv_reader = csv.reader(file)
#    for row in csv_reader:
#        for word in row:
#            if word.strip():
#                words.append(word.strip())

# Commented out IPython magic to ensure Python compatibility.
import torch
import torch.nn.functional as F
import matplotlib.pyplot as plt # for making figures
# %matplotlib inline

#words = open('names.txt', 'r').read().splitlines()
words = words_corpus[:139000]

(len(words) * 100) / len(words_corpus)

# build the vocabulary of characters and mappings to/from integers
chars = sorted(list(set(''.join(words))))
stoi = {s:i+1 for i,s in enumerate(chars)}
stoi['.'] = 0
itos = {i:s for s,i in stoi.items()}
print(itos)

len(itos)

# build the dataset
block_size = 3 # context length: how many characters do we take to predict the next one?

def build_dataset(words):
  X, Y = [], []
  for w in words:

    #print(w)
    context = [0] * block_size
    for ch in w + '.':
      ix = stoi[ch]
      X.append(context)
      Y.append(ix)
      #print(''.join(itos[i] for i in context), '--->', itos[ix])
      context = context[1:] + [ix] # crop and append

  X = torch.tensor(X)
  Y = torch.tensor(Y)
  print(X.shape, Y.shape)
  return X, Y

import random
random.seed(42)
random.shuffle(words)
n1 = int(0.8*len(words))
n2 = int(0.9*len(words))

Xtr, Ytr = build_dataset(words[:n1])
Xdev, Ydev = build_dataset(words[n1:n2])
Xte, Yte = build_dataset(words[n2:])

g = torch.Generator().manual_seed(2147483647) # for reproducibility
C = torch.randn((27, 10), generator=g)
W1 = torch.randn((30, 200), generator=g)
b1 = torch.randn(200, generator=g)
W2 = torch.randn((200, 27), generator=g)
b2 = torch.randn(27, generator=g)
parameters = [C, W1, b1, W2, b2]

sum(p.nelement() for p in parameters) # number of parameters in total

for p in parameters:
  p.requires_grad = True

lre = torch.linspace(-3, 0, 1000)
lrs = 10**lre

lri = []
lossi = []
stepi = []

for i in range(200000):

  # batch here, change
  # minibatch construct
  ix = torch.randint(0, Xtr.shape[0], (32,))

  # forward pass
  emb = C[Xtr[ix]] # (32, 3, 2)
  h = torch.tanh(emb.view(-1, 30) @ W1 + b1) # (32, 100)
  logits = h @ W2 + b2 # (32, 27)
  loss = F.cross_entropy(logits, Ytr[ix])
  #print(loss.item())

  # backward pass
  for p in parameters:
    p.grad = None
  loss.backward()

  # changes here

  # update
  #lr = lrs[i]
  lr = 0.1 if i < 100000 else 0.01
  for p in parameters:
    p.data += -lr * p.grad

  # track stats
  #lri.append(lre[i])
  stepi.append(i)
  lossi.append(loss.log10().item())

#print(loss.item())

plt.plot(stepi, lossi)

# training loss
emb = C[Xtr] # (32, 3, 2)
h = torch.tanh(emb.view(-1, 30) @ W1 + b1) # (32, 100)
logits = h @ W2 + b2 # (32, 27)
loss = F.cross_entropy(logits, Ytr)
loss

# validation loss
emb = C[Xdev] # (32, 3, 2)
h = torch.tanh(emb.view(-1, 30) @ W1 + b1) # (32, 100)
logits = h @ W2 + b2 # (32, 27)
loss = F.cross_entropy(logits, Ydev)
loss

# test loss
emb = C[Xte] # (32, 3, 2)
h = torch.tanh(emb.view(-1, 30) @ W1 + b1) # (32, 100)
logits = h @ W2 + b2 # (32, 27)
loss = F.cross_entropy(logits, Yte)
loss

# visualize dimensions 0 and 1 of the embedding matrix C for all characters
plt.figure(figsize=(8,8))
plt.scatter(C[:,0].data, C[:,1].data, s=200)
for i in range(C.shape[0]):
    plt.text(C[i,0].item(), C[i,1].item(), itos[i], ha="center", va="center", color='white')
plt.grid('minor')



# sample from the model
g = torch.Generator().manual_seed(2147483647 + 10)

predicted_words = []

for _ in range(1000):

    out = []
    context = [0] * block_size # initialize with all ...
    while True:
      emb = C[torch.tensor([context])] # (1,block_size,d)
      h = torch.tanh(emb.view(1, -1) @ W1 + b1)
      logits = h @ W2 + b2
      probs = F.softmax(logits, dim=1)
      ix = torch.multinomial(probs, num_samples=1, generator=g).item()
      context = context[1:] + [ix]
      out.append(ix)
      if ix == 0:
        break

    list_out = [''.join(itos[i] for i in out)]
    predicted_words.append(list_out)
    #print(''.join(itos[i] for i in out))

import pandas as pd
# Calling DataFrame constructor on list
df1 = pd.DataFrame(predicted_words)

# Rename col
df1 = df1.rename(columns={0: 'word'})

# Remove dots from values in 'col1'
df1['word'] = df1['word'].str.rstrip('.')

#df1

matches_df = df1.isin(set(words_corpus[139000:]))

in_corpus_df = df1.isin(set(words_corpus[:139000]))
in_corpus_df = in_corpus_df.rename(columns={'word': 'in_training'})

# Count the number of True values in 'col1'
count_of_trues = (matches_df['word'] == True).sum()
matches_df = matches_df.rename(columns={'word': 'test_set'})

print(count_of_trues)
print("Accuracy:", (count_of_trues * 100) / len(predicted_words), "%")

concatenated_df = pd.concat([df1, matches_df, in_corpus_df], axis=1)
concatenated_df

# Filtering the DataFrame based on the conditions
result = concatenated_df[(concatenated_df['match'] == True) & (concatenated_df['in_training'] == False)]

# Count the number of True values in 'col1'
count_of_trues = (result['match'] == True).sum()
print(count_of_trues)
print("Accuracy", (count_of_trues * 100) / len(predicted_words), "%")

result

# @title in_training

from matplotlib import pyplot as plt
import seaborn as sns
concatenated_df.groupby('in_training').size().plot(kind='barh', color=sns.palettes.mpl_palette('Set1'))
plt.gca().spines[['top', 'right',]].set_visible(False)

# @title match

from matplotlib import pyplot as plt
import seaborn as sns
concatenated_df.groupby('test_set').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)

# @title match vs in_training

from matplotlib import pyplot as plt
import seaborn as sns
import pandas as pd
plt.subplots(figsize=(8, 8))
df_2dhist = pd.DataFrame({
    x_label: grp['in_training'].value_counts()
    for x_label, grp in concatenated_df.groupby('test_set')
})
sns.heatmap(df_2dhist, cmap='viridis')
plt.xlabel('test_set')
_ = plt.ylabel('in_training')

"""## iterations

# others
"""

df2 = pd.DataFrame(set(words_corpus[139000:]))
df2 = df2.rename(columns={0: 'word'})
df2

# Compare the two DataFrames column by column
comparison = df1.equals(df2)

print(comparison)

# Merge the two DataFrames on the common column
merged_df = pd.merge(df1, df2, on='word', how='inner')
merged_df